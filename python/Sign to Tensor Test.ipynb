{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수화 동영상 파일 -> 텐서 파일(+ 미디어파이프 처리 영상 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensro_and_video(input_video_path, output_tensor_path, output_video_path, video_save=False):\n",
    "    \n",
    "    # Prepare DrawingSpec\n",
    "    mp_drawing = mp.solutions.drawing_utils \n",
    "    drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "    # Config holistic\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    # 영상 가져오기\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # 영상 저장 1\n",
    "    if video_save:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V') # 영상 포맷\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (1280,720)) # 비디오 경로, 영상 포맷, 초당 프레임, width*height \n",
    "\n",
    "    # 영상 width, height 설정\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    # 각 요소(왼손, 오른손, 얼굴, 포즈) 좌표 저장 리스트\n",
    "    left_hand_lists = []\n",
    "    right_hand_lists = []\n",
    "    face_lists = []\n",
    "    pose_lists = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "\n",
    "        success, image = cap.read() \n",
    "\n",
    "        if not success: # 동영상 끝\n",
    "            break\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Draw landmark annotation on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "        # 왼손 랜드마크 리스트에 저장\n",
    "        if results.left_hand_landmarks: # 영상에 왼손이 잡힐 경우\n",
    "            left_hand_list = []\n",
    "            for lm in results.left_hand_landmarks.landmark:\n",
    "                left_hand_list.append([lm.x, lm.y, lm.z])\n",
    "            left_hand_lists.append(left_hand_list)\n",
    "        else: # 영상에 오른손이 잡히지 않을 경우\n",
    "            left_hand_lists.append([[0] * 3 for _ in range(21)])\n",
    "              \n",
    "        # 오른손 랜드마크 리스트에 저장\n",
    "        if results.right_hand_landmarks: # 영상에 오른손이 잡힐 경우\n",
    "            right_hand_list = []\n",
    "            for lm in results.right_hand_landmarks.landmark:\n",
    "                right_hand_list.append([lm.x, lm.y, lm.z])\n",
    "            right_hand_lists.append(right_hand_list)\n",
    "        else: # 영상에 오른손이 잡히지 않을 경우\n",
    "            right_hand_lists.append([[0] * 3 for _ in range(21)])\n",
    "\n",
    "        # 얼굴 랜드마크 리스트에 저장\n",
    "        if results.face_landmarks: # 영상에 얼굴이 잡힐 경우\n",
    "            face_list = []\n",
    "            for lm in results.face_landmarks.landmark:\n",
    "                face_list.append([lm.x, lm.y, lm.z])\n",
    "            face_lists.append(face_list)\n",
    "        else: # 영상에 얼굴이 잡히지 않을 경우\n",
    "            face_lists.append([[0] * 3 for _ in range(468)])\n",
    "\n",
    "        # 포즈 랜드마크 리스트에 저장\n",
    "        if results.pose_landmarks: # 영상에 포즈가 잡힐 경우\n",
    "            pose_list = []\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                pose_list.append([lm.x, lm.y, lm.z])\n",
    "            pose_lists.append(pose_list)\n",
    "        else: # 영상에 포즈가 잡히지 않을 경우\n",
    "            pose_lists.append([[0] * 3 for _ in range(33)])\n",
    "            \n",
    "        # 영상 저장 2 (실질적인 영상 쓰기)\n",
    "        if video_save:\n",
    "            out.write(image)\n",
    "\n",
    "    # 텐서로 변환\n",
    "    lt = torch.FloatTensor(left_hand_lists)\n",
    "    rt = torch.FloatTensor(right_hand_lists)\n",
    "    ft = torch.FloatTensor(face_lists)\n",
    "    pt = torch.FloatTensor(pose_lists)\n",
    "    \n",
    "    # 텐서 저장할 디렉토리 생성\n",
    "    if not(os.path.isdir(output_tensor_path)):\n",
    "        os.mkdir(output_tensor_path)\n",
    "        \n",
    "    # 텐서 파일 저장\n",
    "    torch.save(lt, output_tensor_path+\"lt.pt\")\n",
    "    torch.save(rt, output_tensor_path+\"rt.pt\")\n",
    "    torch.save(ft, output_tensor_path+\"ft.pt\")\n",
    "    torch.save(pt, output_tensor_path+\"pt.pt\")\n",
    "\n",
    "    \n",
    "    # 영상 저장 3\n",
    "    if video_save:\n",
    "        out.release()\n",
    "        \n",
    "    holistic.close()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 디렉토리 생성 함수\n",
    "def mkdirs(file_path_list: list, verbose=True):\n",
    "    # ex) file_path_list= [\".\", \"output\", \"images\"]\n",
    "    \n",
    "    file_path = \"\"  # 생성할 디렉토리\n",
    "    for path in file_path_list:\n",
    "        file_path = os.path.join(file_path, path)\n",
    "        \n",
    "        # 디렉토리가 없다면 생성\n",
    "        if not(os.path.isdir(file_path)):\n",
    "            # 디렉토리 생성 안내 문구 출력\n",
    "            if verbose:\n",
    "                print(\"No {}\".format(file_path))\n",
    "                print(\"Make directory {}\".format(file_path))\n",
    "            # 디렉토리 생성\n",
    "            os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_videos_to_tensor(input_video_path_list=[\".\", \"videos\"],\n",
    "                             output_tensor_path_list=[\".\", \"output\", \"tensor\"],\n",
    "                             output_video_path_list=[\".\", \"output\", \"video\"],\n",
    "                             videos_save=False):\n",
    "    \n",
    "    # 디렉토리 풀내임 생성\n",
    "    input_video_path = os.path.join(*input_video_path_list)\n",
    "    output_tensor_path = os.path.join(*output_tensor_path_list)\n",
    "    if videos_save:\n",
    "        output_video_path = os.path.join(*output_video_path_list)\n",
    "\n",
    "    # 안내 문구 출력\n",
    "    print(\"{:20}{}\".format(\"intput_video_path: \", input_video_path))\n",
    "    print(\"{:20}{}\".format(\"output_tensor_path: \", output_tensor_path))\n",
    "    print(\"{:20}{}\".format(\"videos_save: \", str(videos_save)))\n",
    "    if videos_save:\n",
    "        print(\"{:20}{}\".format(\"output_video_path: \", output_video_path))\n",
    "    print()\n",
    "    \n",
    "    # 디렉토리 생성\n",
    "    mkdirs(input_video_path_list)\n",
    "    mkdirs(output_tensor_path_list)\n",
    "    if videos_save:\n",
    "        mkdirs(output_video_path_list)\n",
    "\n",
    "    # 각 비디오 파일 \n",
    "    videos = os.listdir(input_video_path)\n",
    "    videos.sort()\n",
    "\n",
    "    for i, video in enumerate(videos):\n",
    "        make_tensor_and_video(\"./videos/\"+sign_video,\n",
    "                              \"./output/tensor/\"+sign_video[:-4]+\"/\",\n",
    "                              \"./output/videos/\"+sign_video,\n",
    "                             videos_save)\n",
    "        if i%5==0:\n",
    "            print(\"{}/{} videos completed\".format(i, len(videos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intput_video_path:  ./videos\n",
      "output_tensor_path: ./output/tensor\n",
      "videos_save:        False\n",
      "\n",
      "0/51 videos completed\n",
      "5/51 videos completed\n",
      "10/51 videos completed\n",
      "15/51 videos completed\n",
      "20/51 videos completed\n",
      "25/51 videos completed\n",
      "30/51 videos completed\n",
      "35/51 videos completed\n",
      "40/51 videos completed\n",
      "45/51 videos completed\n",
      "50/51 videos completed\n"
     ]
    }
   ],
   "source": [
    "# 동영상 개장 3~5초 정도 소요\n",
    "# ex) 동영상 50개: 4분 소요\n",
    "convert_videos_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 21, 3])\n",
      "torch.Size([160, 21, 3])\n",
      "torch.Size([160, 33, 3])\n",
      "torch.Size([160, 468, 3])\n"
     ]
    }
   ],
   "source": [
    "# shape 확인\n",
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/lt.pt\").shape)\n",
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/rt.pt\").shape)\n",
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/pt.pt\").shape)\n",
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/ft.pt\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4906,  0.2412, -0.0145],\n",
      "         [ 0.4891,  0.2166, -0.0271],\n",
      "         [ 0.4897,  0.2243, -0.0141],\n",
      "         ...,\n",
      "         [ 0.4974,  0.1712, -0.0031],\n",
      "         [ 0.5260,  0.1595,  0.0065],\n",
      "         [ 0.5284,  0.1572,  0.0066]],\n",
      "\n",
      "        [[ 0.4911,  0.2400, -0.0150],\n",
      "         [ 0.4897,  0.2175, -0.0268],\n",
      "         [ 0.4902,  0.2248, -0.0140],\n",
      "         ...,\n",
      "         [ 0.4969,  0.1719, -0.0024],\n",
      "         [ 0.5247,  0.1603,  0.0076],\n",
      "         [ 0.5271,  0.1578,  0.0077]],\n",
      "\n",
      "        [[ 0.4913,  0.2394, -0.0154],\n",
      "         [ 0.4902,  0.2161, -0.0269],\n",
      "         [ 0.4905,  0.2238, -0.0142],\n",
      "         ...,\n",
      "         [ 0.4972,  0.1717, -0.0020],\n",
      "         [ 0.5247,  0.1602,  0.0083],\n",
      "         [ 0.5271,  0.1577,  0.0084]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4902,  0.2401, -0.0154],\n",
      "         [ 0.4884,  0.2171, -0.0269],\n",
      "         [ 0.4892,  0.2247, -0.0143],\n",
      "         ...,\n",
      "         [ 0.4957,  0.1719, -0.0020],\n",
      "         [ 0.5233,  0.1599,  0.0076],\n",
      "         [ 0.5257,  0.1574,  0.0077]],\n",
      "\n",
      "        [[ 0.4901,  0.2401, -0.0153],\n",
      "         [ 0.4883,  0.2175, -0.0270],\n",
      "         [ 0.4890,  0.2249, -0.0143],\n",
      "         ...,\n",
      "         [ 0.4953,  0.1721, -0.0023],\n",
      "         [ 0.5231,  0.1597,  0.0075],\n",
      "         [ 0.5255,  0.1572,  0.0077]],\n",
      "\n",
      "        [[ 0.4905,  0.2401, -0.0154],\n",
      "         [ 0.4890,  0.2169, -0.0269],\n",
      "         [ 0.4896,  0.2245, -0.0142],\n",
      "         ...,\n",
      "         [ 0.4961,  0.1721, -0.0021],\n",
      "         [ 0.5237,  0.1603,  0.0078],\n",
      "         [ 0.5261,  0.1579,  0.0079]]])\n"
     ]
    }
   ],
   "source": [
    "# face 좌표 확인\n",
    "ft = torch.load(\"./output/tensor/KETI_SL_0000000002/ft.pt\")\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
