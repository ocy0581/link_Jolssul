{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "from protobuf_to_dict import protobuf_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Video_and_tensor(input_path, output_video_path, output_tensor_path):\n",
    "\n",
    "    # Prepare DrawingSpec\n",
    "    mp_drawing = mp.solutions.drawing_utils \n",
    "    drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "    # Config holistic\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    # 영상 가져오기\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # 영상 저장\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V') # 영상 포맷\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (1280,720)) # 비디오 경로, 영상 포맷, 초당 프레임, width*height \n",
    "\n",
    "    # 영상 width, height 설정\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    # 각 요소(왼손, 오른손, 얼굴, 포즈) 좌표 저장 리스트\n",
    "    left_hand_lists = []\n",
    "    right_hand_lists = []\n",
    "    face_lists = []\n",
    "    pose_lists = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "\n",
    "        success, image = cap.read() \n",
    "\n",
    "        if not success: # 동영상 끝\n",
    "            break\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Draw landmark annotation on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "        # 왼손 랜드마크 리스트에 저장\n",
    "        if results.left_hand_landmarks: # 영상에 왼손이 잡힐 경우\n",
    "            left_hand_list = []\n",
    "            for lm in results.left_hand_landmarks.landmark:\n",
    "                left_hand_list.append([lm.x, lm.y, lm.z])\n",
    "            left_hand_lists.append(left_hand_list)\n",
    "        else: # 영상에 오른손이 잡히지 않을 경우\n",
    "            left_hand_lists.append([[0] * 3 for _ in range(21)])\n",
    "              \n",
    "        # 오른손 랜드마크 리스트에 저장\n",
    "        if results.right_hand_landmarks: # 영상에 오른손이 잡힐 경우\n",
    "            right_hand_list = []\n",
    "            for lm in results.right_hand_landmarks.landmark:\n",
    "                right_hand_list.append([lm.x, lm.y, lm.z])\n",
    "            right_hand_lists.append(right_hand_list)\n",
    "        else: # 영상에 오른손이 잡히지 않을 경우\n",
    "            right_hand_lists.append([[0] * 3 for _ in range(21)])\n",
    "\n",
    "        # 얼굴 랜드마크 리스트에 저장\n",
    "        if results.face_landmarks: # 영상에 얼굴이 잡힐 경우\n",
    "            face_list = []\n",
    "            for lm in results.face_landmarks.landmark:\n",
    "                face_list.append([lm.x, lm.y, lm.z])\n",
    "            face_lists.append(face_list)\n",
    "        else: # 영상에 얼굴이 잡히지 않을 경우\n",
    "            face_lists.append([[0] * 3 for _ in range(468)])\n",
    "\n",
    "        # 포즈 랜드마크 리스트에 저장\n",
    "        if results.pose_landmarks: # 영상에 포즈가 잡힐 경우\n",
    "            pose_list = []\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                pose_list.append([lm.x, lm.y, lm.z])\n",
    "            pose_lists.append(pose_list)\n",
    "        else: # 영상에 포즈가 잡히지 않을 경우\n",
    "            pose_lists.append([[0] * 3 for _ in range(33)])\n",
    "            \n",
    "\n",
    "        out.write(image)  # 실질적인 영상 쓰기   \n",
    "\n",
    "    # 텐서로 변환\n",
    "    lt = torch.FloatTensor(left_hand_lists)\n",
    "    rt = torch.FloatTensor(right_hand_lists)\n",
    "    ft = torch.FloatTensor(face_lists)\n",
    "    pt = torch.FloatTensor(pose_lists)\n",
    "    \n",
    "    # 텐서 저장할 디렉토리 생성\n",
    "    if not(os.path.isdir(output_tensor_path)):\n",
    "        os.mkdir(output_tensor_path)\n",
    "        \n",
    "    # 텐서 파일 저장\n",
    "    torch.save(lt, output_tensor_path+\"lt.pt\")\n",
    "    torch.save(rt, output_tensor_path+\"rt.pt\")\n",
    "    torch.save(ft, output_tensor_path+\"ft.pt\")\n",
    "    torch.save(pt, output_tensor_path+\"pt.pt\")\n",
    "    \n",
    "    holistic.close()\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 디렉토리 생성\n",
    "if not(os.path.isdir(\"./output/video\")):\n",
    "    os.mkdir(\"./output/video\")\n",
    "if not(os.path.isdir(\"./output/tensor\")):\n",
    "    os.mkdir(\"./output/tensor\")\n",
    "\n",
    "for SignVideo in os.listdir(\"./video\"):\n",
    "    make_Video_and_tensor(\"./video/\"+SignVideo, \"./output/video/\"+SignVideo, \"./output/tensor/\"+SignVideo[:-4]+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 21, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/lt.pt\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 21, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/rt.pt\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 33, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/pt.pt\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 468, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load(\"./output/tensor/KETI_SL_0000000002/ft.pt\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
