{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수화 동영상 파일 -> 텐서 파일(+ 미디어파이프 처리 영상 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모션 랜드마크를 리스트로 반환\n",
    "def convert_landmark_to_tensor(left_hand_landmarks, right_hand_landmarks):\n",
    "    # 영상에 모션이 잡힐 경우 \n",
    "    if left_hand_landmarks or right_hand_landmarks: \n",
    "        motion_location = []\n",
    "        \n",
    "        # 왼손 랜드마크\n",
    "        if left_hand_landmarks:\n",
    "            for left_lm in left_hand_landmarks.landmark:\n",
    "                motion_location.append([left_lm.x, left_lm.y, left_lm.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \n",
    "        # 오른손 랜드마크\n",
    "        if right_hand_landmarks:\n",
    "            for right_lm in right_hand_landmarks.landmark:\n",
    "                motion_location.append([right_lm.x, right_lm.y, right_lm.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \n",
    "        return motion_location\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모션 랜드마크를 리스트로 반환\n",
    "def convert_landmark_to_tensor(left_hand_landmarks, right_hand_landmarks, pose_landmarks):\n",
    "    # 영상에 모션이 잡힐 경우 \n",
    "    if left_hand_landmarks or right_hand_landmarks: \n",
    "        motion_location = []\n",
    "        \n",
    "        # 왼손 랜드마크\n",
    "        if left_hand_landmarks:\n",
    "            for left_lm in left_hand_landmarks.landmark:\n",
    "                motion_location.append([left_lm.x, left_lm.y, left_lm.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \n",
    "        # 오른손 랜드마크\n",
    "        if right_hand_landmarks:\n",
    "            for right_lm in right_hand_landmarks.landmark:\n",
    "                motion_location.append([right_lm.x, right_lm.y, right_lm.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \"\"\"   \n",
    "        if pose_landmarks:\n",
    "            for pose in pose_landmarks.landmark:\n",
    "                motion_location.append([pose.x, pose.y, pose.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \"\"\"\n",
    "        \n",
    "        return motion_location\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor_and_video(input_video_path, output_tensor_path, output_video_path, video_save=False):\n",
    "    \n",
    "    # Prepare DrawingSpec\n",
    "    mp_drawing = mp.solutions.drawing_utils \n",
    "    drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "    # Config holistic\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    # 영상 가져오기\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # 영상 저장 1\n",
    "    if video_save:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V') # 영상 포맷\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (1280,720)) # 비디오 경로, 영상 포맷, 초당 프레임, width*height \n",
    "\n",
    "    # 영상 width, height 설정\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    # 각 요소(왼손, 오른손, 얼굴, 포즈) 좌표 저장 리스트\n",
    "    #hand_list = []\n",
    "    hp_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "\n",
    "        success, image = cap.read() \n",
    "\n",
    "        if not success: # 동영상 끝\n",
    "            break\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Draw landmark annotation on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "        \n",
    "        # 각 랜드마크를 리스트로 변환 후 리스트에 저장\n",
    "        #landmark = convert_landmark_to_tensor(results.left_hand_landmarks, results.right_hand_landmarks)\n",
    "        landmark = convert_landmark_to_tensor(results.left_hand_landmarks, results.right_hand_landmarks, results.pose_landmarks)\n",
    "        if landmark:\n",
    "            #hand_list.append(landmark)\n",
    "            hp_list.append(landmark)\n",
    "        \n",
    "        # 영상 저장 2 (실질적인 영상 쓰기)\n",
    "        if video_save:\n",
    "            out.write(image)\n",
    "\n",
    "    # 텐서로 변환\n",
    "    #hand_tensor = torch.FloatTensor(hand_list)\n",
    "    hp_tensor = torch.FloatTensor(hp_list) \n",
    "\n",
    "    # 텐서를 저장할 디렉토리 생성\n",
    "    if not(os.path.isdir(output_tensor_path)):\n",
    "        os.makedirs(output_tensor_path)\n",
    "        \n",
    "    # 텐서 파일 저장\n",
    "    torch.save(hp_tensor, os.path.join(output_tensor_path, \"hand.pt\"))\n",
    "\n",
    "    if video_save:\n",
    "        # 동영사을 저장할 디렉토리 생성\n",
    "        if not(os.path.isdir(output_video_path)):\n",
    "            os.mkdir(output_video_path)\n",
    "        \n",
    "        # 영상 저장 3\n",
    "        if video_save:\n",
    "            out.release()\n",
    "\n",
    "    holistic.close()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 디렉토리 생성 함수\n",
    "def mkdirs(file_path_list: list, verbose=True):\n",
    "    # ex) file_path_list= [\".\", \"output\", \"images\"]\n",
    "    \n",
    "    file_path = \"\"  # 생성할 디렉토리\n",
    "    for path in file_path_list:\n",
    "        file_path = os.path.join(file_path, path)\n",
    "        \n",
    "        # 디렉토리가 없다면 생성\n",
    "        if not(os.path.isdir(file_path)):\n",
    "            # 디렉토리 생성 안내 문구 출력\n",
    "            if verbose:\n",
    "                print(\"No {}\".format(file_path))\n",
    "                print(\"Make directory {}\".format(file_path))\n",
    "                print()\n",
    "                \n",
    "            # 디렉토리 생성\n",
    "            os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_videos_to_tensor(input_video_path_list=[\".\", \"videos\"],\n",
    "                             output_tensor_path_list=[\".\", \"output\", \"tensor\"],\n",
    "                             output_video_path_list=[\".\", \"output\", \"video\"],\n",
    "                             videos_save=False):\n",
    "    \n",
    "    # csv파일 로드\n",
    "    \"\"\"\n",
    "    csv1 = pd.read_csv(\"output\\csv\\KETI-2017-SL-0_10480-v2_1.csv\")\n",
    "    csv2 = pd.read_csv(\"output\\csv\\KETI-2018-SL-Annotation-v1.csv\")\n",
    "\n",
    "    dir_names = pd.concat([csv1['파일명'], csv2['파일명']], ignore_index=True)\n",
    "    dir_names = dir_names[:43492]\n",
    "\n",
    "    kors = pd.concat([csv1['한국어'], csv2['한국어']], ignore_index=True)\n",
    "    kors = kors[:43492]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 디렉토리 풀내임 생성\n",
    "    input_video_path = os.path.join(*input_video_path_list)\n",
    "    output_tensor_path = os.path.join(*output_tensor_path_list)\n",
    "    output_video_path = os.path.join(*output_video_path_list)\n",
    "\n",
    "    # 안내 문구 출력\n",
    "    print(\"{:20}{}\".format(\"input_video_path: \", input_video_path))\n",
    "    print(\"{:20}{}\".format(\"output_tensor_path: \", output_tensor_path))\n",
    "    print(\"{:20}{}\".format(\"videos_save: \", str(videos_save)))\n",
    "    \n",
    "    if videos_save:\n",
    "        print(\"{:20}{}\".format(\"output_video_path: \", output_video_path))\n",
    "    print()\n",
    "    \n",
    "    # 디렉토리 생성\n",
    "    mkdirs(input_video_path_list)\n",
    "    mkdirs(output_tensor_path_list)\n",
    "    if videos_save:\n",
    "        mkdirs(output_video_path_list)\n",
    "\n",
    "    # 비디오 폴더\n",
    "    dir_video = os.listdir(input_video_path)\n",
    "    dir_video.sort()\n",
    "    \n",
    "    # 각 비디오 폴더 순환\n",
    "    for video_folder in dir_video:\n",
    "        print(\"{:20}{}\".format(\"current_video_folder: \", video_folder))\n",
    "        final_input_video_path = input_video_path+\"\\\\\"+video_folder\n",
    "        videos = os.listdir(final_input_video_path)\n",
    "        if video_folder == 'temp':\n",
    "            continue\n",
    "        \n",
    "        for i, video in enumerate(videos):\n",
    "                make_tensor_and_video(os.path.join(input_video_path, video_folder, video),\n",
    "                                      os.path.join(output_tensor_path, video_folder, video[:-4]),  # [:-4]: 확장자 제거\n",
    "                                      os.path.join(output_video_path, video),\n",
    "                                      videos_save)\n",
    "                if i%5==0:\n",
    "                    print(\"{}/{} videos completed\".format(i, len(videos)))\n",
    "    \n",
    "    print('------------------------------------------')\n",
    "    print(\"Completed!\")\n",
    "    print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_video_path:   .\\videos\n",
      "output_tensor_path: .\\output\\tensor\n",
      "videos_save:        False\n",
      "\n",
      "current_video_folder: prediction\n",
      "0/4 videos completed\n",
      "current_video_folder: temp\n",
      "------------------------------------------\n",
      "Completed!\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 동영상 개장 3~5초 정도 소요\n",
    "# ex) 동영상 50개: 4분 소요\n",
    "convert_videos_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
