{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수화 동영상 파일 -> 텐서 파일(+ 미디어파이프 처리 영상 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모션 랜드마크를 리스트로 반환\n",
    "def convert_landmark_to_tensor(left_hand_landmarks, right_hand_landmarks):\n",
    "    # 영상에 모션이 잡힐 경우 \n",
    "    if left_hand_landmarks or right_hand_landmarks: \n",
    "        motion_location = []\n",
    "        \n",
    "        # 왼손 랜드마크\n",
    "        if left_hand_landmarks:\n",
    "            for left_lm in left_hand_landmarks.landmark:\n",
    "                motion_location.append([left_lm.x, left_lm.y, left_lm.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \n",
    "        # 오른손 랜드마크\n",
    "        if right_hand_landmarks:\n",
    "            for right_lm in right_hand_landmarks.landmark:\n",
    "                motion_location.append([right_lm.x, right_lm.y, right_lm.z])\n",
    "        else:\n",
    "            for i in range(21):\n",
    "                motion_location.append([0, 0, 0])\n",
    "        \n",
    "        return motion_location\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor_and_video(input_video_path, output_tensor_path, output_video_path, video_save=False):\n",
    "    \n",
    "    # Prepare DrawingSpec\n",
    "    mp_drawing = mp.solutions.drawing_utils \n",
    "    drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "    # Config holistic\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    # 영상 가져오기\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # 영상 저장 1\n",
    "    if video_save:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V') # 영상 포맷\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (1280,720)) # 비디오 경로, 영상 포맷, 초당 프레임, width*height \n",
    "\n",
    "    # 영상 width, height 설정\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    # 각 요소(왼손, 오른손, 얼굴, 포즈) 좌표 저장 리스트\n",
    "    hand_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "\n",
    "        success, image = cap.read() \n",
    "\n",
    "        if not success: # 동영상 끝\n",
    "            break\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Draw landmark annotation on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        \n",
    "        # 각 랜드마크를 리스트로 변환 후 리스트에 저장\n",
    "        landmark = convert_landmark_to_tensor(results.left_hand_landmarks, results.right_hand_landmarks)\n",
    "        if landmark:\n",
    "            hand_list.append(landmark)\n",
    "        \n",
    "        # 영상 저장 2 (실질적인 영상 쓰기)\n",
    "        if video_save:\n",
    "            out.write(image)\n",
    "\n",
    "    # 텐서로 변환\n",
    "    hand_tensor = torch.FloatTensor(hand_list)\n",
    "        \n",
    "    # 텐서를 저장할 디렉토리 생성\n",
    "    if not(os.path.isdir(output_tensor_path)):\n",
    "        os.makedirs(output_tensor_path)\n",
    "        \n",
    "    # 텐서 파일 저장\n",
    "    torch.save(hand_tensor, os.path.join(output_tensor_path, \"hand.pt\"))\n",
    "\n",
    "    if video_save:\n",
    "        # 동영사을 저장할 디렉토리 생성\n",
    "        if not(os.path.isdir(output_video_path)):\n",
    "            os.mkdir(output_video_path)\n",
    "        \n",
    "        # 영상 저장 3\n",
    "        if video_save:\n",
    "            out.release()\n",
    "\n",
    "    holistic.close()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 디렉토리 생성 함수\n",
    "def mkdirs(file_path_list: list, verbose=True):\n",
    "    # ex) file_path_list= [\".\", \"output\", \"images\"]\n",
    "    \n",
    "    file_path = \"\"  # 생성할 디렉토리\n",
    "    for path in file_path_list:\n",
    "        file_path = os.path.join(file_path, path)\n",
    "        \n",
    "        # 디렉토리가 없다면 생성\n",
    "        if not(os.path.isdir(file_path)):\n",
    "            # 디렉토리 생성 안내 문구 출력\n",
    "            if verbose:\n",
    "                print(\"No {}\".format(file_path))\n",
    "                print(\"Make directory {}\".format(file_path))\n",
    "                print()\n",
    "                \n",
    "            # 디렉토리 생성\n",
    "            os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_videos_to_tensor(input_video_path_list=[\".\", \"videos\"],\n",
    "                             output_tensor_path_list=[\".\", \"output\", \"tensor\"],\n",
    "                             output_video_path_list=[\".\", \"output\", \"video\"],\n",
    "                             videos_save=False):\n",
    "    \n",
    "    # csv파일 로드\n",
    "    \"\"\"\n",
    "    csv1 = pd.read_csv(\"output\\csv\\KETI-2017-SL-0_10480-v2_1.csv\")\n",
    "    csv2 = pd.read_csv(\"output\\csv\\KETI-2018-SL-Annotation-v1.csv\")\n",
    "\n",
    "    dir_names = pd.concat([csv1['파일명'], csv2['파일명']], ignore_index=True)\n",
    "    dir_names = dir_names[:43492]\n",
    "\n",
    "    kors = pd.concat([csv1['한국어'], csv2['한국어']], ignore_index=True)\n",
    "    kors = kors[:43492]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 디렉토리 풀내임 생성\n",
    "    input_video_path = os.path.join(*input_video_path_list)\n",
    "    output_tensor_path = os.path.join(*output_tensor_path_list)\n",
    "    output_video_path = os.path.join(*output_video_path_list)\n",
    "\n",
    "    # 안내 문구 출력\n",
    "    print(\"{:20}{}\".format(\"input_video_path: \", input_video_path))\n",
    "    print(\"{:20}{}\".format(\"output_tensor_path: \", output_tensor_path))\n",
    "    print(\"{:20}{}\".format(\"videos_save: \", str(videos_save)))\n",
    "    \n",
    "    if videos_save:\n",
    "        print(\"{:20}{}\".format(\"output_video_path: \", output_video_path))\n",
    "    print()\n",
    "    \n",
    "    # 디렉토리 생성\n",
    "    mkdirs(input_video_path_list)\n",
    "    mkdirs(output_tensor_path_list)\n",
    "    if videos_save:\n",
    "        mkdirs(output_video_path_list)\n",
    "\n",
    "    # 비디오 폴더\n",
    "    dir_video = os.listdir(input_video_path)\n",
    "    dir_video.sort()\n",
    "    \n",
    "    # 각 비디오 폴더 순환\n",
    "    for video_folder in dir_video:\n",
    "        print(\"{:20}{}\".format(\"current_video_folder: \", video_folder))\n",
    "        final_input_video_path = input_video_path+\"\\\\\"+video_folder\n",
    "        videos = os.listdir(final_input_video_path)\n",
    "        \n",
    "        for i, video in enumerate(videos):\n",
    "                make_tensor_and_video(os.path.join(input_video_path, video_folder, video),\n",
    "                                      os.path.join(output_tensor_path, video_folder, video[:-4]),  # [:-4]: 확장자 제거\n",
    "                                      os.path.join(output_video_path, video),\n",
    "                                      videos_save)\n",
    "                if i%5==0:\n",
    "                    print(\"{}/{} videos completed\".format(i, len(videos)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_video_path:   .\\videos\n",
      "output_tensor_path: .\\output\\tensor\n",
      "videos_save:        False\n",
      "\n",
      "current_video_folder: num\n",
      "0/8 videos completed\n",
      "5/8 videos completed\n"
     ]
    }
   ],
   "source": [
    "# 동영상 개장 3~5초 정도 소요\n",
    "# ex) 동영상 50개: 4분 소요\n",
    "convert_videos_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/tensor/KETI_SL_0000000011/hand.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-989166d7bbe9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# shape 확인\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./output/tensor/KETI_SL_0000000011/hand.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/tensor/KETI_SL_0000000011/hand.pt'"
     ]
    }
   ],
   "source": [
    "# shape 확인\n",
    "print(torch.load(\"./output/tensor/KETI_SL_0000000011/hand.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 가장 바깥 리스트만 텐서로 바뀌고 텐서 내부는 전부 리스트임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
