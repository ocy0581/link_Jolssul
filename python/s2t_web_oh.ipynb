{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모션 랜드마크를 리스트로 반환\n",
    "def convert_landmark_to_tensor(landmarks, n_point):\n",
    "    # 영상에 모션이 잡힐 경우 \n",
    "    if landmarks: \n",
    "        motion_location = []\n",
    "        for lm in landmarks.landmark:\n",
    "            motion_location.append([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        return motion_location\n",
    "        \n",
    "    # 영상에 모션이 잡히지 않을 경우\n",
    "    else:\n",
    "        return [[0] * 3 for _ in range(n_point)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.0\n",
      "30.0\n",
      "time : 17.525031566619873\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이거 쓰세용     \n",
    "\n",
    "\"\"\"\n",
    "keypoints = []\n",
    "left_hands = []\n",
    "mp_holistic = mp.solutions.holistic\n",
    "video_hol = mp.solutions.holistic.Holistic(static_image_mode=False,upper_body_only=False,smooth_landmarks=True,\n",
    "                                           min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "# Prepare DrawingSpec for drawing the face landmarks later.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# For webcam input:\n",
    "holistic = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"videos\\KETI_SL_0000000001.avi\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FPS, 1)\n",
    "\n",
    "total_frames = cap.get(7)\n",
    "print(total_frames)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "frame_rate = 10\n",
    "prev = 0\n",
    "i = 0\n",
    "left_hand_list = []\n",
    "right_hand_list = []\n",
    "face_list = []\n",
    "pose_list = []\n",
    "\n",
    "start = time.time()\n",
    "while cap.isOpened():\n",
    "#     time_elapsed = time.time() - prev\n",
    "    success, image = cap.read()\n",
    "\n",
    "#     if time_elapsed > 1./frame_rate:\n",
    "#         prev = time.time()\n",
    "    # print(type(image))\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "    # If loading a video, use 'break' instead of 'continue'.\n",
    "        break\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = holistic.process(image)\n",
    "    # Draw landmark annotation on the image.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "    \n",
    "    # 각 랜드마크를 리스트로 변환 후 리스트에 저장\n",
    "    left_hand_list.append(convert_landmark_to_tensor(results.left_hand_landmarks, 21))\n",
    "    right_hand_list.append(convert_landmark_to_tensor(results.right_hand_landmarks, 21))\n",
    "    face_list.append(convert_landmark_to_tensor(results.face_landmarks, 468))\n",
    "    pose_list.append(convert_landmark_to_tensor(results.pose_landmarks, 33))\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    cv2.imshow('MediaPipe Holistic', image)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "      \n",
    "    if cv2.waitKey(5) & 0xFF == ord('w'):\n",
    "        left_hand_list = []\n",
    "        right_hand_list = []\n",
    "        face_list = []\n",
    "        pose_list = []\n",
    "        break\n",
    "holistic.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(t):\n",
    "    t= t[:120] # seq_size가 변동되는걸 처리 못해서 임시로 고정해둠\n",
    "    return t.view(1,-1,t.shape[1]*t.shape[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = pipe(torch.FloatTensor(left_hand_list))\n",
    "rt = pipe(torch.FloatTensor(right_hand_list))\n",
    "ft = pipe(torch.FloatTensor(face_list))\n",
    "pt = pipe(torch.FloatTensor(pose_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 120, 63]) torch.Size([1, 120, 63]) torch.Size([1, 120, 1404]) torch.Size([1, 120, 99])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lt.shape, rt.shape, ft.shape, pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadtensor(numRange):\n",
    "\n",
    "    lt_list = []\n",
    "    rt_list = []\n",
    "    ft_list = []\n",
    "    pt_list = []\n",
    "    tmp_base = \"0000000000\"\n",
    "    for x in range(1,numRange+1):\n",
    "        base = tmp_base[:-1*(len(str(x)))]\n",
    "\n",
    "        lt_list.append(torch.load(\"./output/tensor/KETI_SL_\"+base+str(x)+\"/left_hand.pt\"))\n",
    "        rt_list.append(torch.load(\"./output/tensor/KETI_SL_\"+base+str(x)+\"/right_hand.pt\"))\n",
    "        ft_list.append(torch.load(\"./output/tensor/KETI_SL_\"+base+str(x)+\"/face.pt\"))\n",
    "        pt_list.append(torch.load(\"./output/tensor/KETI_SL_\"+base+str(x)+\"/pose.pt\"))\n",
    "    return lt_list,rt_list,ft_list,pt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_list,rt_list,ft_list,pt_list = loadtensor(20)\n",
    "lt_list = list(map(pipe,lt_list))\n",
    "rt_list = list(map(pipe,rt_list))\n",
    "ft_list = list(map(pipe,ft_list))\n",
    "pt_list = list(map(pipe,pt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    '''\n",
    "    현재 모델 상황\n",
    "    \n",
    "    구조\n",
    "    left, right, face, pose 전부 각각을 rnn -> dnn 순으로 입력후\n",
    "    모두 concat하고 flatten 한뒤 dnn에 입력함\n",
    "\n",
    "    rnn -> relu -> dnn -> relu -> concat -> flat -> dnn -> softmax 순\n",
    "\n",
    "    구성 이유\n",
    "    왼손, 오른손, 얼굴, 포즈를 각각 rnn에 돌려서 특정 패턴으로 인식하고 \n",
    "    해당 패턴을 조합하여 값을 내놓는다 라는 가정하에 만들었음\n",
    "\n",
    "    제한 사항\n",
    "\n",
    "    input shape를 고정하고\n",
    "    output 또한 one hot vector로 출력되며 해당값은 1~20사이 값만 출력됨\n",
    "\n",
    "    수정이 필요한 사항\n",
    "    1.  seq사이즈를 고정하지 않고 하려 했으내 nn.Embeding은 long타입이 필요하며\n",
    "        다른 방식으로 seq사이즈를 고정하는 방식을 찾이 못했음\n",
    "\n",
    "    2.  output이 지금은 softmax를 활용한 단순 분류임 text로 변환하려는 작업이 추가적으로 필요함\n",
    "\n",
    "    '''\n",
    "    def __init__(self,seq, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        # (batch_size,seq, 21*3)\n",
    "        # (batch_size,seq, 21*3)\n",
    "        # (batch_size,seq, 468*3)\n",
    "        # (batch_size,seq, 33*3)\n",
    "        self.lt = torch.nn.RNN(21*3, hidden_size,batch_first=True) # RNN 셀 구현\n",
    "        self.rt = torch.nn.RNN(21*3, hidden_size,batch_first=True) # RNN 셀 구현\n",
    "        self.ft = torch.nn.RNN(468*3, hidden_size,batch_first=True) # RNN 셀 구현\n",
    "        self.pt = torch.nn.RNN(33*3, hidden_size,batch_first=True) # RNN 셀 구현\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.soft = torch.nn.Softmax(dim=1)\n",
    "        self.dnnlt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnrt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnft = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnpt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "\n",
    "        self.flat   = torch.nn.Flatten()\n",
    "        self.outDnn = torch.nn.Linear(seq*output_size*4,20,bias=True)\n",
    "    def forward(self,lt,rt,ft,pt ): # 구현한 RNN 셀과 출력층을 연결\n",
    "        rlt, _status =self.lt(lt)\n",
    "        rrt, _status =self.rt(rt)\n",
    "        rft, _status =self.ft(ft)\n",
    "        rpt, _status =self.pt(pt)\n",
    "\n",
    "        rlt=self.relu(rlt)\n",
    "        rrt=self.relu(rrt)\n",
    "        rft=self.relu(rft)\n",
    "        rpt=self.relu(rpt)\n",
    "\n",
    "\n",
    "        dlt= self.dnnlt(rlt)\n",
    "        drt= self.dnnrt(rrt)\n",
    "        dft= self.dnnft(rft)\n",
    "        dpt= self.dnnpt(rpt)\n",
    "\n",
    "        \n",
    "        dlt=self.relu(dlt)\n",
    "        drt=self.relu(drt)\n",
    "        dft=self.relu(dft)\n",
    "        dpt=self.relu(dpt)\n",
    "\n",
    "        fullDnn = torch.cat((dlt, drt, dft, dpt),dim=2)\n",
    "        out = self.flat(fullDnn)\n",
    "        out = self.outDnn(out)\n",
    "        out = self.soft(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Net.forward of Net(\n",
       "  (lt): RNN(63, 60, batch_first=True)\n",
       "  (rt): RNN(63, 60, batch_first=True)\n",
       "  (ft): RNN(1404, 60, batch_first=True)\n",
       "  (pt): RNN(99, 60, batch_first=True)\n",
       "  (relu): ReLU()\n",
       "  (soft): Softmax(dim=1)\n",
       "  (dnnlt): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (dnnrt): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (dnnft): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (dnnpt): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (outDnn): Linear(in_features=96000, out_features=20, bias=True)\n",
       ")>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "net = Net(lt_list[0].shape[1], 60,200)\n",
    "net.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "loss_fn = torch.nn.BCELoss  # Binary Cross Entropy 강화학습때 했던 log사용하는 loss 함수\n",
    "                            # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "y_data = torch.FloatTensor(np.eye(20))\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-74f063a9a267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch : {0} , loss : {1} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    group['eps'])\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "losses = [] \n",
    "for i in range(epochs): \n",
    "\n",
    "\n",
    "    for x in range(len(lt_list)):\n",
    "        \n",
    "        predict = net(lt_list[x], rt_list[x], ft_list[x], pt_list[x])    # [1,20] ont-hot vector\n",
    "        X = predict[0] # [20] 으로 변경\n",
    "        Y = y_data[x]  # [20] 짜리 단위 행렬\n",
    "\n",
    "        loss = loss_fn()(X,Y)   # Binary Cross Entropy 강화학습때 했던 log사용하는 loss 함수\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    print(\"epoch : {0} , loss : {1} \".format(i,sum(losses[i*20:(i+1)*20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "model = Net(lt_list[0].shape[1], 60,200)\n",
    "model.load_state_dict(torch.load(\"./output/model/model.pt\"),strict=False) # model load 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.2892e-43, 8.2677e-44, 9.8091e-44, 6.4460e-44, 7.4269e-44,\n",
       "         6.1657e-44, 6.5861e-44, 6.7262e-44, 7.8473e-44, 9.1084e-44, 7.8473e-44,\n",
       "         6.3058e-44, 7.4269e-44, 6.7262e-44, 7.7071e-44, 6.5861e-44, 7.8473e-44,\n",
       "         6.5861e-44, 7.9874e-44]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "net(lt_list[x], rt_list[x], ft_list[x], pt_list[x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[3.8699e-09, 2.1393e-09, 2.3582e-08, 5.8050e-09, 4.8973e-08, 2.9401e-08,\n",
       "          1.3573e-09, 4.4913e-08, 1.9066e-09, 1.0000e+00, 2.9191e-09, 1.3987e-09,\n",
       "          1.8078e-09, 7.1477e-09, 2.0885e-09, 9.3883e-10, 2.9283e-09, 2.5023e-10,\n",
       "          1.0199e-09, 8.7668e-09]], grad_fn=<SoftmaxBackward>), 10)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "model(lt_list[x], rt_list[x], ft_list[x], pt_list[x]) , x+1 #x+1인 이유 x는 0부터 시작함 즉 x = 9이면 10에 해당하는것이 1이됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[7.0290e-01, 1.4449e-05, 4.4292e-05, 1.2296e-03, 2.3749e-01, 2.0924e-05,\n",
       "          9.1894e-06, 7.9870e-06, 2.5336e-05, 9.0888e-07, 3.5219e-07, 1.0738e-04,\n",
       "          1.3813e-02, 4.2674e-05, 3.7969e-05, 2.4304e-05, 4.8775e-05, 4.3588e-02,\n",
       "          3.3810e-04, 2.5872e-04]], grad_fn=<SoftmaxBackward>), 10)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "model(lt, rt, ft, pt) , x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "np.argmax(model(lt,rt,ft,pt).detach())"
   ]
  },
  {
   "source": [
    "매 frame마다 tensor를 만들고\n",
    "\n",
    "실제 모델 실행시 90frame을 넣는다라고 가정을하면\n",
    "\n",
    "tensor를 만드는거는 값을 직접입력하는 방식으로 진행하고 \n",
    "\n",
    "만든 tensor를 리스트에 append한다. \n",
    "\n",
    "그 이후 b = torch.Tensor(90,size,3)\n",
    "\n",
    "torch.cat(tensorlist[-90:],out = b)\n",
    "이런식으로 뽑는다\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}