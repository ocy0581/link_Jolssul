{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLandmark(landmarks,landmark_lists : list)->list:\n",
    "\n",
    "    landmark_list = []\n",
    "    for lm in landmarks.landmark:\n",
    "        landmark_list.append([lm.x, lm.y,lm.z])\n",
    "    landmark_lists.append(landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def landmarklist(results,lt,rt,ft,pt):\n",
    "    \n",
    "    \n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        extractLandmark(results.left_hand_landmarks,lt) \n",
    "    else:\n",
    "        lt.append(np.zeros([21,3]))\n",
    "    if results.right_hand_landmarks:\n",
    "        extractLandmark(results.right_hand_landmarks,rt)\n",
    "    else:\n",
    "        rt.append(np.zeros([21,3]))\n",
    "    if results.face_landmarks:\n",
    "        extractLandmark(results.face_landmarks,ft)\n",
    "    else:\n",
    "        ft.append(np.zeros([468, 3]))\n",
    "    if results.pose_landmarks:\n",
    "        extractLandmark(results.pose_landmarks,pt)\n",
    "    else:\n",
    "        pt.append(np.zeros([33, 3]))\n",
    "\n",
    "    return lt, rt, ft, pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "holistic.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.0\n",
      "30.0\n",
      "time : 21.744114637374878\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이거 쓰세용     \n",
    "\n",
    "\"\"\"\n",
    "keypoints = []\n",
    "left_hands = []\n",
    "mp_holistic = mp.solutions.holistic\n",
    "video_hol = mp.solutions.holistic.Holistic(static_image_mode=False,upper_body_only=False,smooth_landmarks=True,\n",
    "                                           min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "# Prepare DrawingSpec for drawing the face landmarks later.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# For webcam input:\n",
    "holistic = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"./video/KETI_SL_0000006020.avi\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FPS, 1)\n",
    "\n",
    "total_frames = cap.get(7)\n",
    "print(total_frames)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "frame_rate = 10\n",
    "prev = 0\n",
    "i = 0\n",
    "left_hand_lists = []\n",
    "right_hand_lists = []\n",
    "face_lists = []\n",
    "pose_lists = []\n",
    "\n",
    "start = time.time()\n",
    "while cap.isOpened():\n",
    "#     time_elapsed = time.time() - prev\n",
    "    success, image = cap.read()\n",
    "\n",
    "#     if time_elapsed > 1./frame_rate:\n",
    "#         prev = time.time()\n",
    "    # print(type(image))\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "    # If loading a video, use 'break' instead of 'continue'.\n",
    "        break\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = holistic.process(image)\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "      \n",
    "    \n",
    "    landmarklist(results,left_hand_lists ,right_hand_lists ,face_lists ,pose_lists)\n",
    "\n",
    "        \n",
    "        \n",
    "    cv2.imshow('MediaPipe Holistic', image)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "holistic.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([189, 21, 3]) torch.Size([189, 21, 3]) torch.Size([189, 468, 3]) torch.Size([189, 33, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lt = torch.FloatTensor(left_hand_lists)\n",
    "rt = torch.FloatTensor(right_hand_lists)\n",
    "ft = torch.FloatTensor(face_lists)\n",
    "pt = torch.FloatTensor(pose_lists)\n",
    "\n",
    "\n",
    "print(\n",
    "torch.FloatTensor(left_hand_lists).shape,\n",
    "torch.FloatTensor(right_hand_lists).shape,\n",
    "torch.FloatTensor(face_lists).shape,\n",
    "torch.FloatTensor(pose_lists).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        # self.lt = torch.nn.RNN((None,21,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        # self.rh = torch.nn.RNN((None,21,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        # self.ft = torch.nn.RNN((None,468,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        # self.pt = torch.nn.RNN((None,33,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "#         [None,21,3]\n",
    "# (None,21,3)\n",
    "# (None,468,3)\n",
    "# (None,33,3)\n",
    "        \n",
    "        self.lt = torch.nn.RNN(input_size, hidden_size) # RNN 셀 구현\n",
    "        self.rh = torch.nn.RNN(input_size, hidden_size) # RNN 셀 구현\n",
    "        self.ft = torch.nn.RNN(input_size, hidden_size) # RNN 셀 구현\n",
    "        self.pt = torch.nn.RNN(input_size, hidden_size) # RNN 셀 구현\n",
    "        self.dnnlt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnrh = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnft = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnpt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.outDnn = torch.nn.Linear(output_size*4,50,bias=True)\n",
    "    def forward(self,lt,rh,ft,pt ): # 구현한 RNN 셀과 출력층을 연결\n",
    "        rlt, _status =self.lt(lt)\n",
    "        rrh, _status =self.rh(rh)\n",
    "        rft, _status =self.ft(ft)\n",
    "        rpt, _status =self.pt(pt)\n",
    "\n",
    "        dlt= self.dnnlt(rlt)\n",
    "        drh= self.dnnrh(rrh)\n",
    "        dft= self.dnnft(rft)\n",
    "        dpt= self.dnnpt(rpt)\n",
    "\n",
    "        fullDnn = torch.cat((dlt, drh, dft, dpt),dim=1)\n",
    "        out = outDnn(fullDnn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = Net(30, 30,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute 'fullDnn'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d46739687f27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfullDnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    946\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 948\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Net' object has no attribute 'fullDnn'"
     ]
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "source": [
    "매 frame마다 tensor를 만들고\n",
    "\n",
    "실제 모델 실행시 90frame을 넣는다라고 가정을하면\n",
    "\n",
    "tensor를 만드는거는 값을 직접입력하는 방식으로 진행하고 \n",
    "\n",
    "만든 tensor를 리스트에 append한다. \n",
    "\n",
    "그 이후 b = torch.Tensor(90,size,3)\n",
    "\n",
    "torch.cat(tensorlist[-90:],out = b)\n",
    "이런식으로 뽑는다\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}