{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLandmark(landmarks,landmark_lists : list)->list:\n",
    "\n",
    "    landmark_list = []\n",
    "    for lm in landmarks.landmark:\n",
    "        landmark_list.append([lm.x, lm.y,lm.z])\n",
    "    landmark_lists.append(landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def landmarklist(results,lt,rt,ft,pt):\n",
    "    \n",
    "    \n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        extractLandmark(results.left_hand_landmarks,lt) \n",
    "    else:\n",
    "        lt.append(np.zeros([21,3]))\n",
    "    if results.right_hand_landmarks:\n",
    "        extractLandmark(results.right_hand_landmarks,rt)\n",
    "    else:\n",
    "        rt.append(np.zeros([21,3]))\n",
    "    if results.face_landmarks:\n",
    "        extractLandmark(results.face_landmarks,ft)\n",
    "    else:\n",
    "        ft.append(np.zeros([468, 3]))\n",
    "    if results.pose_landmarks:\n",
    "        extractLandmark(results.pose_landmarks,pt)\n",
    "    else:\n",
    "        pt.append(np.zeros([33, 3]))\n",
    "\n",
    "    return lt, rt, ft, pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "holistic.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.0\n",
      "30.0\n",
      "time : 14.50534987449646\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이거 쓰세용     \n",
    "\n",
    "\"\"\"\n",
    "keypoints = []\n",
    "left_hands = []\n",
    "mp_holistic = mp.solutions.holistic\n",
    "video_hol = mp.solutions.holistic.Holistic(static_image_mode=False,upper_body_only=False,smooth_landmarks=True,\n",
    "                                           min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "# Prepare DrawingSpec for drawing the face landmarks later.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# For webcam input:\n",
    "holistic = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"./video/KETI_SL_0000006020.avi\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FPS, 1)\n",
    "\n",
    "total_frames = cap.get(7)\n",
    "print(total_frames)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "frame_rate = 10\n",
    "prev = 0\n",
    "i = 0\n",
    "left_hand_lists = []\n",
    "right_hand_lists = []\n",
    "face_lists = []\n",
    "pose_lists = []\n",
    "\n",
    "start = time.time()\n",
    "while cap.isOpened():\n",
    "#     time_elapsed = time.time() - prev\n",
    "    success, image = cap.read()\n",
    "\n",
    "#     if time_elapsed > 1./frame_rate:\n",
    "#         prev = time.time()\n",
    "    # print(type(image))\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "    # If loading a video, use 'break' instead of 'continue'.\n",
    "        break\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = holistic.process(image)\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "      \n",
    "    \n",
    "    landmarklist(results,left_hand_lists ,right_hand_lists ,face_lists ,pose_lists)\n",
    "\n",
    "        \n",
    "        \n",
    "    cv2.imshow('MediaPipe Holistic', image)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "holistic.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([108, 21, 3]) torch.Size([108, 21, 3]) torch.Size([108, 468, 3]) torch.Size([108, 33, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lt = torch.FloatTensor(left_hand_lists)\n",
    "rt = torch.FloatTensor(right_hand_lists)\n",
    "ft = torch.FloatTensor(face_lists)\n",
    "pt = torch.FloatTensor(pose_lists)\n",
    "\n",
    "\n",
    "print(\n",
    "torch.FloatTensor(left_hand_lists).shape,\n",
    "torch.FloatTensor(right_hand_lists).shape,\n",
    "torch.FloatTensor(face_lists).shape,\n",
    "torch.FloatTensor(pose_lists).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(t):\n",
    "    t = t[:15*(t.shape[0]//15)]\n",
    "    return t.reshape(t.shape[0]//15,15,t.shape[1]*t.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = pipe(lt)\n",
    "rt = pipe(rt)\n",
    "ft = pipe(ft)\n",
    "pt = pipe(pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7, 15, 63]) torch.Size([7, 15, 63]) torch.Size([7, 15, 1404]) torch.Size([7, 15, 99])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lt.shape, rt.shape, ft.shape, pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,seq_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        # self.lt = torch.nn.RNN((None,21,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        # self.rt = torch.nn.RNN((None,21,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        # self.ft = torch.nn.RNN((None,468,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        # self.pt = torch.nn.RNN((None,33,3), hidden_size, batch_first=True) # RNN 셀 구현\n",
    "#         [None,21,3]\n",
    "# (None,21,3)\n",
    "# (None,468,3)\n",
    "# (None,33,3)\n",
    "        self.lt = torch.nn.RNN(21*3, hidden_size) # RNN 셀 구현\n",
    "        self.rt = torch.nn.RNN(21*3, hidden_size) # RNN 셀 구현\n",
    "        self.ft = torch.nn.RNN(468*3, hidden_size) # RNN 셀 구현\n",
    "        self.pt = torch.nn.RNN(33*3, hidden_size) # RNN 셀 구현\n",
    "        self.dnnlt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnrt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnft = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.dnnpt = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "        self.outDnn = torch.nn.Linear(output_size,50,bias=True)\n",
    "    def forward(self,lt,rt,ft,pt ): # 구현한 RNN 셀과 출력층을 연결\n",
    "        rlt, _status =self.lt(lt)\n",
    "        rrt, _status =self.rt(rt)\n",
    "        rft, _status =self.ft(ft)\n",
    "        rpt, _status =self.pt(pt)\n",
    "\n",
    "        dlt= self.dnnlt(rlt)\n",
    "        drt= self.dnnrt(rrt)\n",
    "        dft= self.dnnft(rft)\n",
    "        dpt= self.dnnpt(rpt)\n",
    "\n",
    "        fullDnn = torch.cat((dlt, drt, dft, dpt),dim=1)\n",
    "        out = self.outDnn(fullDnn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Net.forward of Net(\n",
       "  (lt): RNN(63, 60)\n",
       "  (rt): RNN(63, 60)\n",
       "  (ft): RNN(1404, 60)\n",
       "  (pt): RNN(99, 60)\n",
       "  (dnnlt): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (dnnrt): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (dnnft): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (dnnpt): Linear(in_features=60, out_features=200, bias=True)\n",
       "  (outDnn): Linear(in_features=200, out_features=50, bias=True)\n",
       ")>"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "\n",
    "net = Net(lt.shape[0], 60,200)\n",
    "net.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7, 15, 63])\n",
      "torch.Size([7, 60, 200])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-0.0037,  0.0078, -0.0582,  ...,  0.0531,  0.1692, -0.0409],\n",
       "         [-0.0037,  0.0078, -0.0582,  ...,  0.0531,  0.1692, -0.0409],\n",
       "         [-0.0037,  0.0078, -0.0582,  ...,  0.0531,  0.1692, -0.0409],\n",
       "         ...,\n",
       "         [-0.0384,  0.1855,  0.1462,  ..., -0.0618,  0.3647, -0.0924],\n",
       "         [-0.0384,  0.1852,  0.1469,  ..., -0.0610,  0.3639, -0.0942],\n",
       "         [-0.0390,  0.1854,  0.1472,  ..., -0.0618,  0.3645, -0.0920]],\n",
       "\n",
       "        [[-0.0221,  0.0268, -0.0264,  ...,  0.0541,  0.2037, -0.0179],\n",
       "         [-0.0221,  0.0268, -0.0264,  ...,  0.0541,  0.2037, -0.0179],\n",
       "         [-0.0221,  0.0268, -0.0264,  ...,  0.0541,  0.2037, -0.0179],\n",
       "         ...,\n",
       "         [ 0.0265,  0.1980,  0.1420,  ..., -0.0070,  0.3368, -0.0846],\n",
       "         [ 0.0247,  0.2004,  0.1554,  ...,  0.0010,  0.3399, -0.0881],\n",
       "         [ 0.0266,  0.1964,  0.1472,  ..., -0.0046,  0.3299, -0.0889]],\n",
       "\n",
       "        [[ 0.0906, -0.0283,  0.1232,  ...,  0.0201,  0.1985, -0.1995],\n",
       "         [ 0.0909, -0.0277,  0.1241,  ...,  0.0199,  0.1990, -0.2003],\n",
       "         [ 0.0902, -0.0270,  0.1234,  ...,  0.0203,  0.1982, -0.1992],\n",
       "         ...,\n",
       "         [ 0.0391,  0.2020,  0.1253,  ..., -0.0321,  0.3253, -0.0997],\n",
       "         [ 0.0378,  0.2062,  0.1271,  ..., -0.0306,  0.3206, -0.1011],\n",
       "         [ 0.0350,  0.2001,  0.1253,  ..., -0.0328,  0.3215, -0.0951]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0615, -0.0579,  0.2139,  ...,  0.0438,  0.1792, -0.1691],\n",
       "         [ 0.0608, -0.0565,  0.2201,  ...,  0.0412,  0.1850, -0.1708],\n",
       "         [ 0.0598, -0.0561,  0.2206,  ...,  0.0409,  0.1863, -0.1730],\n",
       "         ...,\n",
       "         [ 0.0373,  0.1755,  0.1077,  ..., -0.0078,  0.3287, -0.1097],\n",
       "         [ 0.0351,  0.1737,  0.1037,  ..., -0.0113,  0.3305, -0.1071],\n",
       "         [ 0.0377,  0.1759,  0.1034,  ..., -0.0044,  0.3313, -0.1120]],\n",
       "\n",
       "        [[ 0.0715, -0.0598,  0.2293,  ...,  0.0446,  0.1801, -0.1412],\n",
       "         [ 0.0718, -0.0588,  0.2283,  ...,  0.0440,  0.1794, -0.1403],\n",
       "         [ 0.0684, -0.0589,  0.2254,  ...,  0.0498,  0.1753, -0.1444],\n",
       "         ...,\n",
       "         [ 0.0078,  0.1615,  0.1057,  ...,  0.0029,  0.3561, -0.1149],\n",
       "         [-0.0010,  0.1589,  0.1039,  ..., -0.0017,  0.3573, -0.1166],\n",
       "         [ 0.0095,  0.1607,  0.1144,  ...,  0.0029,  0.3557, -0.1144]],\n",
       "\n",
       "        [[-0.0181, -0.0235,  0.0898,  ...,  0.0967,  0.1488,  0.0048],\n",
       "         [-0.0180, -0.0235,  0.0899,  ...,  0.0955,  0.1485,  0.0062],\n",
       "         [-0.0210, -0.0216,  0.0906,  ...,  0.0922,  0.1566,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0108,  0.1536,  0.1063,  ...,  0.0167,  0.3611, -0.1000],\n",
       "         [ 0.0103,  0.1556,  0.1084,  ...,  0.0176,  0.3589, -0.1017],\n",
       "         [ 0.0072,  0.1569,  0.1049,  ...,  0.0145,  0.3623, -0.1015]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "print(lt.shape)\n",
    "\n",
    "net(lt,rt,ft,pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7, 15, 63])\n"
     ]
    }
   ],
   "source": [
    "tmp = lt\n",
    "tmp.reshape([108,63])\n",
    "t = tmp[:105].reshape([105,63])\n",
    "t.shape\n",
    "t = t.reshape(lt.shape[0]//15,15,lt.shape[1]*lt.shape[2])\n",
    "print(t.shape)\n"
   ]
  },
  {
   "source": [
    "매 frame마다 tensor를 만들고\n",
    "\n",
    "실제 모델 실행시 90frame을 넣는다라고 가정을하면\n",
    "\n",
    "tensor를 만드는거는 값을 직접입력하는 방식으로 진행하고 \n",
    "\n",
    "만든 tensor를 리스트에 append한다. \n",
    "\n",
    "그 이후 b = torch.Tensor(90,size,3)\n",
    "\n",
    "torch.cat(tensorlist[-90:],out = b)\n",
    "이런식으로 뽑는다\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}